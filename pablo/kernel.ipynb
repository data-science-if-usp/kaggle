{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 0.24.2\n",
      "numpy: 1.16.2\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For preprocessing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "print(\"pandas: {}\".format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22e4089694a1384b977bb5695eb14cc863704f05"
   },
   "source": [
    "# Load Data as dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5175bb2beb69025872faf02dd926bddb98b441ba"
   },
   "outputs": [],
   "source": [
    "shops_df = pd.read_csv('../input/shops.csv', dtype={'shop_name': 'str', 'shop_id': 'int32'})  \n",
    "                            # columns: ['shop_name','shop_id']\n",
    "item_cat_df = pd.read_csv('../input/item_categories.csv', \n",
    "                              dtype={'item_category_name': 'str', 'item_category_id': 'int32'}) \n",
    "                            # columns: ['item_category_name', 'item_category_id']\n",
    "sales_train_df  = pd.read_csv('../input/sales_train.csv',parse_dates=['date'], \n",
    "                    dtype={'date': 'str', 'date_block_num': 'int32', 'shop_id': 'int32', \n",
    "                          'item_id': 'int32', 'item_price': 'float32', 'item_cnt_day': 'int32'}) \n",
    "                            # ['date','date_block_num','shop_id','item_id','item_price', 'item_cnt_day']\n",
    "items_df = pd.read_csv('../input/items.csv', dtype={'item_name': 'str', 'item_id': 'int32', \n",
    "                                                 'item_category_id': 'int32'})  \n",
    "                            # ['item_name','item_id','item_category_id']\n",
    "sample_sub_df = pd.read_csv('../input/sample_submission.csv')  # ['ID',item_cnt_month']\n",
    "test_df = pd.read_csv('../input/test.csv',dtype={'ID': 'int32', 'shop_id': 'int32', 'item_id': 'int32'}) # [shop_id','item_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a68a8abc6c0488a44f4fca7c20cf2b35e922ad9"
   },
   "source": [
    "## Ideas: \n",
    "After a little EDA and some thought, I've arrived to the following ideas about this problem:\n",
    "*  Use `['shop_id','item_id']` as the index, it is more convenient (I believe, let's see)\n",
    "*  Compare the train and test datasets to understand how they were split - this will help with the private score-\n",
    "*  You need some criteria to fill in the missing values in the train dataset. I believe this is achived through feature engineering.\n",
    "* We will start with Decission Trees, XGBoost, Random Forest, linear regression and knn regression. In all cases (excpet by knn maybe) we will need a lot of features, so we will start with feature creation. Remember that we are dealing with time series, this means that time is a relevant variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c831618cc0c9535cdcbd20dc61459548aacce91"
   },
   "source": [
    "Let's compare the pairs `['shop_id','item_id']` in train and test datasets, there are a few things to note:\n",
    "  * There are no pairs for `'shop_id'=0,1`. This might, at first, suggest that we could get rid of those pairs in the training set. Look for the evolution of stores through time, since there are closings and openings of stores, this could lead to reducing the number of shops in considereation.\n",
    "  * There are pairs in test that do not appear in train, this arises the question of whether and how to include those pairs into the train dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab19773b05241d6d1c6d05c1ecb8119b41f16301"
   },
   "outputs": [],
   "source": [
    "# Unique ['shop_id','item_id'] pairs in train dataset\n",
    "train_pairs = sales_train_df[['shop_id','item_id']].sort_values(['shop_id','item_id']).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Unique ['shop_id','item_id'] pairs in train dataset\n",
    "test_pairs = test_df[['shop_id','item_id']].sort_values(['shop_id','item_id']).drop_duplicates().reset_index(drop=True)\n",
    "print(\"Number of train unique pairs: {}\".format(train_pairs.shape))\n",
    "print(\"Number of test unique pairs: {}\".format(test_pairs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70fd717131a60b69ee478e0c9771df37817b5436"
   },
   "outputs": [],
   "source": [
    "# Create the pairs for train set\n",
    "train_pairs['shop_item'] = pd.Series([(train_pairs['shop_id'][i],train_pairs['item_id'][i]) for i in train_pairs.index])\n",
    "\n",
    "# and for test set:\n",
    "test_pairs['shop_item'] = pd.Series([(test_pairs['shop_id'][i],test_pairs['item_id'][i]) for i in test_pairs.index])\n",
    "\n",
    "# we can actually drop the first two columns and we're left with the pair index\n",
    "train_pairs.drop(['shop_id','item_id'], axis = 1, inplace = True)\n",
    "test_pairs.drop(['shop_id','item_id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ae38c1062712ecef60fbfd1a8123836badc5f64f"
   },
   "source": [
    "## Let's construct a (mental) Venn Diagram of the indices\n",
    "We do this by identifying:\n",
    "*  Common indices or `good_pairs`$: =  (\\text{train} \\cap \\text{test})$, but more importantly:\n",
    "*  `train_not_test`$:= \\text{train} - (\\text{train} \\cap \\text{test})$ , this is, indices in `train` that are not in `test`,  and \n",
    "*  `test_not_train`$:= \\text{test} - (\\text{train} \\cap \\text{test})$, or  indices in `test` that are not in `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "453ed642af9825900674ef04d91f76536c8b88b8"
   },
   "outputs": [],
   "source": [
    "good_pairs = train_pairs.merge(test_pairs)\n",
    "train_not_test =  train_pairs[~train_pairs['shop_item'].isin(test_pairs['shop_item'])]\n",
    "test_not_train = test_pairs[~test_pairs['shop_item'].isin(train_pairs['shop_item'])]\n",
    "## Test:\n",
    "test_not_train.isin(train_not_test)['shop_item'].any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19536e9409e3d01f027ce7b7ec0b4a1e7c8e1a9d"
   },
   "outputs": [],
   "source": [
    "print('------------ (shop_id,item_id) ---------')\n",
    "print('unique pairs in test: {}'.format(len(test_pairs)))\n",
    "print('unique pairs in train: {}'.format(len(train_pairs)))\n",
    "print('good_pairs: {}'.format(len(good_pairs)))\n",
    "print('train_not_test: {}'.format(len(train_not_test)))\n",
    "print('test_not_train: {}'.format(len(test_not_train)))\n",
    "print('ratio of test not in train: {}'.format(len(test_not_train)/len(test_pairs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "334f846391788a99ecc22587f74ee9360a0327f9"
   },
   "source": [
    "Observe that the number of unique`test_pairs` is 214200, about half the number of unique pairs in `train_pairs`. This caught my attention since the test set corresponds to a single month of sales (which we need to eventually predict haha) and it has lots of unique pairs or unique events. The ratio between test not in train reveals that ~ 50% of the test pairs (indices) are not there in the train set. Making predictions for these **missing values** is, I think, the trickiest task in this problem.\n",
    "\n",
    "The test_not_train indices are split in two subsets by looking at the item_id index only, as shown at the end of  [Konstantin Yakovlev's kernel](https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e48e663823836f4f6dddd1af08c7c308684c206"
   },
   "source": [
    "# Strategy\n",
    "I want to have a prediction by the end of the day, in this sense:\n",
    " * We will start with a detailed EDA of the dataset, the purpose of this is twofold:  first, we want to augment the dataset to exhibit correlations between features to whichever model we might use to make predictions and secondly, we need to find criteria to fill the **missing values**.\n",
    " * Perform Feature Engineering, you can look at [Dimitre Oliveira's kernel](https://www.kaggle.com/dimitreoliveira/model-stacking-feature-engineering-and-eda) or [Denis Larionov's one](https://www.kaggle.com/dlarionov/feature-engineering-xgboost) for insights about this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3107b2bbe20e72bfbde1ad60a2d5ec678668bd56"
   },
   "source": [
    "Recall that there is a big mismatch between the `'shop_id'` , `'item_id'` pairs. There are almost 50% pairs missing in the train dataframe and we need to include them for training. \n",
    "  * We first note that the number of items that are not in the train dataset is: 363\n",
    "  * So, in principle, we need to add 363*(number of active shops) items in the train dataset, this is, we're augmenting the data for the new items\n",
    "  * Train set has all shop_ids so we don't need to fill them in.\n",
    " To create the 'complete' train dataset we could create a new index with all `'item_id'`'s by merging with the test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd0a24a358b47e8c7bb2d75a76616310eba44967"
   },
   "outputs": [],
   "source": [
    "shop_unique = pd.DataFrame(sorted(sales_train_df['shop_id'].unique()))\n",
    "item_unique = pd.DataFrame(sorted(sales_train_df['item_id'].unique()))\n",
    "missing_items = test_df[~test_df['item_id'].isin(sales_train_df['item_id'].unique())]['item_id'].unique()\n",
    "all_items = list(set(sales_train_df['item_id']).union(missing_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "4cb0f6b8595dc0c5b967cdcc8e5c35153fb62281"
   },
   "outputs": [],
   "source": [
    "print('number of items in train: {}'.format(len(item_unique)))\n",
    "print('number of missing items: {}'.format(len(missing_items)))\n",
    "print('number of total items: {}'.format(len(all_items)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d319b8e01b3c26c7d7b9ef54241af666b769bac2"
   },
   "source": [
    "## By shop_id:\n",
    "We will create monthly based time series for each store adding `item_cnt_day`. \n",
    "\n",
    "(A mean encoded feature of this kind could be of good use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a430d9ea233a77d8d634b9783cfc3d13f8c16bcc"
   },
   "outputs": [],
   "source": [
    "sales_by_shop = sales_train_df.pivot_table(index = ['shop_id'], values = ['item_cnt_day'], columns = ['date_block_num'], \n",
    "                                           aggfunc=np.sum, fill_value = 0).reset_index()\n",
    "sales_by_shop.columns = sales_by_shop.columns.droplevel().map(str)\n",
    "sales_by_shop = sales_by_shop.reset_index(drop=True).rename_axis(None, axis=1)\n",
    "sales_by_shop.columns.values[0] = 'shop_id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ac7db916c12bb40b1cd42876f3eaa88b6eafd08"
   },
   "outputs": [],
   "source": [
    "sales_by_shop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa4d7670e49f42b57ef355707c748097cbc253ca"
   },
   "source": [
    "From the above pivot table we can see that there are shops with 0 sells for long periods of time. We could identify several cases, for instance: shops 0 and 1 seem to have closed after month 2.  On the other hand shop 48 seems to have been opened in month 15 and tehre could be sales peaks for opening shops (shop 36 could have this feature). Furthermore, note shop 9 that apperas to sell during single particular months, which makes me think that it could be a 'itinerant' shop. Not sure if we want to get rid of those.\n",
    " * closed shops $\\rightarrow$ get rid\n",
    " * newly opened shops and itinerant shops $\\rightarrow$ mantain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a68efa1c5eadc8dcadc1564fbc289019525c857a"
   },
   "outputs": [],
   "source": [
    "# There are 60 shops, let's plot them side by side\n",
    "fig, ax = plt.subplots(5,2, figsize=(30, 15))\n",
    "ax[0,0].plot(sales_by_shop.columns[1:], sales_by_shop.iloc[:6,1:].T, 'o-' )\n",
    "ax[1,0].plot(sales_by_shop.columns[1:], sales_by_shop.iloc[6:12,1:].T, 'o-' )\n",
    "ax[2,0].plot(sales_by_shop.columns[1:], sales_by_shop.iloc[12:18,1:].T, 'o-')\n",
    "ax[3,0].plot(sales_by_shop.columns[1:], sales_by_shop.iloc[18:24,1:].T, 'o-')\n",
    "ax[4,0].plot(sales_by_shop.columns[1:], sales_by_shop.iloc[24:30,1:].T, 'o-')\n",
    "ax[0,1].plot(sales_by_shop.columns[1:], sales_by_shop.iloc[30:36,1:].T, 'o-')\n",
    "ax[1,1].plot(sales_by_shop.columns[1:], sales_by_shop.iloc[36:42,1:].T, 'o-')\n",
    "ax[2,1].plot(sales_by_shop.columns[1:], sales_by_shop.iloc[42:48,1:].T, 'o-')\n",
    "ax[3,1].plot(sales_by_shop.columns[1:], sales_by_shop.iloc[48:54,1:].T, 'o-')\n",
    "ax[4,1].plot(sales_by_shop.columns[1:], sales_by_shop.iloc[54:,1:].T, 'o-')\n",
    "fig.suptitle('Montlhy sales per shop_id', fontsize = 14)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be58d6dcde39fd3b3f609579a245ea41243e6251"
   },
   "source": [
    "Seasonality is self evident in this dataset, seen as the peaks in monthly sales per shop also present in the entire company sales records. Notice that the above figures do not share the y-axis and this is done for visualization purposes only. This can be corrected by setting `sharey='all'` inside the `plt.subplots` function. Also, notice that:\n",
    "* There are a few shops that sell much more than the rest of the company. \n",
    "* Some shops close and others open during the whole time lapse.\n",
    "* Interestingly, there are shops that sell only during the high season, these could correspond to isle sales points or something of the kind.\n",
    "\n",
    "The time evolution of sales for the entire company can be easily shown using the `'sales_by_shop'` pivot table dataframe, from which we can clearly see there is a down trend and the seasonality of sales is self evident. Moreover, the standard error bars show that the top season has greater variance than the rest of the year. Bottomline, while seasonality is evident, the top selling months might be more difficult to predict (by shop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb54cbf2957a031a560dd32e54f5429249df22c1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.errorbar(sales_by_shop.columns[1:], sales_by_shop.iloc[:,1:].mean(), \n",
    "             yerr=sales_by_shop.iloc[:,1:].std()/sales_by_shop.iloc[:,1:].count().add(-1).pow(0.5), fmt='-o', ecolor='orangered',capsize=3)\n",
    "plt.title('Average Monthly Sales', fontsize=14)\n",
    "plt.xlabel('Month', fontsize= 12 )\n",
    "plt.ylabel('Sales', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3a21845c99abbf68eb8629e712b0991ea0b64b2"
   },
   "source": [
    "## By Category_id:\n",
    "Let us now look at the category_id variable, first of all we need to merge the`sales_train_df` dataset with the `items_df` one in order to include the category id of each item in `sales_train_df`.\n",
    "\n",
    "There is also a `item_cat_df` dataset that has specific information about each category in stock, we should try to use it since it can be used to identify similar categories, this in order to augment the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7acb902ba35fe2d9c3d787de4dd31fbdd5c5354e"
   },
   "outputs": [],
   "source": [
    "new = item_cat_df['item_category_name'].str.split(' - ', expand =True)\n",
    "new[1]=new[1].fillna('none')\n",
    "#Label encoding these two new columns\n",
    "le1 = preprocessing.LabelEncoder()\n",
    "le1.fit(new[0].unique())\n",
    "new[2] = le1.transform(new[0])\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "le2.fit(new[1].unique())\n",
    "new[3] = le2.transform(new[1])\n",
    "\n",
    "# Create a copy and fill it with the new columns\n",
    "item_cat_exp = item_cat_df.copy() \n",
    "item_cat_exp['cat_type'] = new[0]\n",
    "item_cat_exp['item_type'] = new[1]\n",
    "item_cat_exp['cat_type_l'] = new[2]\n",
    "item_cat_exp['item_type_l'] = new[3]\n",
    "item_cat_exp.drop('item_category_name', axis =1, inplace= True)\n",
    "item_cat_exp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78499acc6026fed3bb75f8dd3c60ed3774b777fc"
   },
   "source": [
    "Using the `item_cat_exp` dataframe we can explore more about the nature of sells in the company, this is, aggregate sells by category_id and then by cat_type and item_type, they are new features !!! \n",
    "\n",
    "These new features are categorical, one hot encoding could be an option (very expensive one) for these features, maybe label encoding is enough.\n",
    " * There are only 20 category types and 61 item_types\n",
    " * Not all combinations are possible and the possible ones make up the 84 different category pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f454a727a7d969c315e697cf15c504de18972acb"
   },
   "outputs": [],
   "source": [
    "item_cat_exp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "325638a7e44aeecaf89c186140bffd3c6f123b32"
   },
   "source": [
    "Let us construt a dataframe containing the info we extracted above, we will use this dataframe to explore monthly sales as a function of `'item_category_id'`, `'cat_type'`, `'item_type'` and some other feature we may find relevant regarding the information of item's categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6667ed64e8b2073b20324ef5d3069d8e310cad35"
   },
   "outputs": [],
   "source": [
    "sales_cat = sales_train_df.join(items_df, on = 'item_id', rsuffix='_').join(item_cat_exp, on = 'item_category_id', rsuffix = \"_\").drop(['item_name','item_id_','item_category_id_'], axis =1)\n",
    "sales_cat.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a28cdbce4cf3fab560a1a41848b41b2c4508893"
   },
   "outputs": [],
   "source": [
    "g_sales_by_cat = sales_cat.sort_values('date').groupby(['date_block_num', 'cat_type', 'cat_type_l', 'item_category_id','item_id'], as_index = False).agg({'item_price': ['mean','min','max'], 'item_cnt_day': ['sum', 'mean']})\n",
    "g_sales_by_cat.tail(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "08ac8d981c88ce2008a5da6335b883f7e3de3468"
   },
   "outputs": [],
   "source": [
    "# Let's pivot the above dataset to get a monthly time series:\n",
    "sales_by_cat = sales_cat.pivot_table(index=['item_category_id'], values = ['item_cnt_day'], columns=['date_block_num'], \n",
    "                     aggfunc = np.sum, fill_value = 0 ).reset_index()\n",
    "sales_by_cat.columns = sales_by_cat.columns.droplevel().map(str)\n",
    "sales_by_cat = sales_by_cat.reset_index(drop=True).rename_axis(None, axis=1)\n",
    "sales_by_cat.columns.values[0]= 'item_category_id'\n",
    "sales_by_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a10cd43d0aa7cc18252e444ba7a2a86a68d7d539"
   },
   "outputs": [],
   "source": [
    "item_cat_df.item_category_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "704745fce163cf7136d0d3747175ba1563fb8295"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6,2,figsize = (30,15))\n",
    "ax[0][0].plot(sales_by_cat.columns[1:], sales_by_cat.iloc[:8,1:].T, 'o-')\n",
    "ax[1][0].plot(sales_by_cat.columns[1:], sales_by_cat.iloc[8:16,1:].T, 'o-')\n",
    "ax[2][0].plot(sales_by_cat.columns[1:], sales_by_cat.iloc[16:24,1:].T, 'o-')\n",
    "ax[3][0].plot(sales_by_cat.columns[1:], sales_by_cat.iloc[24:32,1:].T, 'o-')\n",
    "ax[4][0].plot(sales_by_cat.columns[1:], sales_by_cat.iloc[32:40,1:].T, 'o-')\n",
    "ax[5][0].plot(sales_by_cat.columns[1:], sales_by_cat.iloc[40:48,1:].T, 'o-')\n",
    "ax[0][1].plot(sales_by_cat.columns[1:], sales_by_cat.iloc[48:56,1:].T, 'o-')\n",
    "ax[1][1].plot(sales_by_cat.columns[1:], sales_by_cat.iloc[56:64,1:].T, 'o-')\n",
    "ax[2][1].plot(sales_by_cat.columns[1:], sales_by_cat.iloc[64:72,1:].T, 'o-')\n",
    "ax[3][1].plot(sales_by_cat.columns[1:], sales_by_cat.iloc[72:80,1:].T, 'o-')\n",
    "ax[4][1].plot(sales_by_cat.columns[1:], sales_by_cat.iloc[80:,1:].T, 'o-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9ec7600bc56d0cf7434e07488326facf14b7fab"
   },
   "source": [
    "The above figures show the time evolution of sales per category. We can identify the behavior of each cateogery in the above graph, for instance, there are categories that outperform the rest. While there is some information in the above figures, definitely it is not the best way to visualize data by category.\n",
    "\n",
    "Let us group data by month and item category while aggregating `item_price` and `item_cnt_day` so that we can see time evolution of those variables. In the end I want to determine the best way of encoding category-related features for our model. Furthermore, we want to use category-related information to come up with criteria for filling in **missing values** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e733d65e47098c67e6f3adb4484654e18354f839"
   },
   "outputs": [],
   "source": [
    "price_by_cat = sales_cat.pivot_table(index=['item_category_id'], values = ['item_price'], columns=['date_block_num'], \n",
    "                     aggfunc = np.mean, fill_value = 0 ).reset_index()\n",
    "price_by_cat.columns = price_by_cat.columns.droplevel().map(str)\n",
    "price_by_cat = price_by_cat.reset_index(drop=True).rename_axis(None, axis=1)\n",
    "price_by_cat.columns.values[0]= 'item_category_id'\n",
    "price_by_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a770a5d53ddcfff4b181618085fc81122d726f4"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6,2,figsize = (30,15))\n",
    "ax[0][0].plot(price_by_cat.columns[1:], price_by_cat.iloc[:8,1:].T, 'o-')\n",
    "ax[1][0].plot(price_by_cat.columns[1:], price_by_cat.iloc[8:16,1:].T, 'o-')\n",
    "ax[2][0].plot(price_by_cat.columns[1:], price_by_cat.iloc[16:24,1:].T, 'o-')\n",
    "ax[3][0].plot(price_by_cat.columns[1:], price_by_cat.iloc[24:32,1:].T, 'o-')\n",
    "ax[4][0].plot(price_by_cat.columns[1:], price_by_cat.iloc[32:40,1:].T, 'o-')\n",
    "ax[5][0].plot(price_by_cat.columns[1:], price_by_cat.iloc[40:48,1:].T, 'o-')\n",
    "ax[0][1].plot(price_by_cat.columns[1:], price_by_cat.iloc[48:56,1:].T, 'o-')\n",
    "ax[1][1].plot(price_by_cat.columns[1:], price_by_cat.iloc[56:64,1:].T, 'o-')\n",
    "ax[2][1].plot(price_by_cat.columns[1:], price_by_cat.iloc[64:72,1:].T, 'o-')\n",
    "ax[3][1].plot(price_by_cat.columns[1:], price_by_cat.iloc[72:80,1:].T, 'o-')\n",
    "ax[4][1].plot(price_by_cat.columns[1:], price_by_cat.iloc[80:,1:].T, 'o-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "def2284dadd1804be4057899a5b1cb9da9ed5bd4"
   },
   "source": [
    "### (average) Price_by_category:\n",
    "The above drawings show that price is not constant at all times and there might be abrupt changes on price due to promos, for instance. This of course is being (mean) aggregated by category_id and a more detailed behavior  is found by looking at data by `item_id` by `cat_type` or `item_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8a56fac5730b05d95114ecc2d20316267cf7356"
   },
   "outputs": [],
   "source": [
    "avg_sales_by_cat = sales_cat.pivot_table(index=['item_category_id'], values = ['item_cnt_day'], columns=['date_block_num'], \n",
    "                     aggfunc = np.mean, fill_value = 0 ).reset_index()\n",
    "avg_sales_by_cat.columns = avg_sales_by_cat.columns.droplevel().map(str)\n",
    "avg_sales_by_cat = avg_sales_by_cat.reset_index(drop=True).rename_axis(None, axis=1)\n",
    "avg_sales_by_cat.columns.values[0]= 'item_category_id'\n",
    "avg_sales_by_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f259754ca964b8e691aa5e96e73183659f5ab08"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6,2,figsize = (30,15))\n",
    "ax[0][0].plot(avg_sales_by_cat.columns[1:], avg_sales_by_cat.iloc[:8,1:].T, 'o-')\n",
    "ax[1][0].plot(avg_sales_by_cat.columns[1:], avg_sales_by_cat.iloc[8:16,1:].T, 'o-')\n",
    "ax[2][0].plot(avg_sales_by_cat.columns[1:], avg_sales_by_cat.iloc[16:24,1:].T, 'o-')\n",
    "ax[3][0].plot(avg_sales_by_cat.columns[1:], avg_sales_by_cat.iloc[24:32,1:].T, 'o-')\n",
    "ax[4][0].plot(avg_sales_by_cat.columns[1:], avg_sales_by_cat.iloc[32:40,1:].T, 'o-')\n",
    "ax[5][0].plot(avg_sales_by_cat.columns[1:], avg_sales_by_cat.iloc[40:48,1:].T, 'o-')\n",
    "ax[0][1].plot(avg_sales_by_cat.columns[1:], avg_sales_by_cat.iloc[48:56,1:].T, 'o-')\n",
    "ax[1][1].plot(avg_sales_by_cat.columns[1:], avg_sales_by_cat.iloc[56:64,1:].T, 'o-')\n",
    "ax[2][1].plot(avg_sales_by_cat.columns[1:], avg_sales_by_cat.iloc[64:72,1:].T, 'o-')\n",
    "ax[3][1].plot(avg_sales_by_cat.columns[1:], avg_sales_by_cat.iloc[72:80,1:].T, 'o-')\n",
    "ax[4][1].plot(avg_sales_by_cat.columns[1:], avg_sales_by_cat.iloc[80:,1:].T, 'o-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3341a65b31a4b3678661844e7379bec74d7a70db"
   },
   "source": [
    "### avg_sales_by_category:\n",
    "We could use this mean encoded feature (It is less sparse than total sales).\n",
    "## By Category_id: Bottomline\n",
    "Category_id information holds information about the evolution of sales and price of items, they will be used as features and possibly as part of the criteria used to fill in data.\n",
    "Possible Features:\n",
    "   * Average monthly sales\n",
    "   * Total monthly sales\n",
    "   * Average monthly price\n",
    "Above we construct the dataset (`cat_price_cnt`) that will be merged into the train set at the end of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95076ff1d3c523a1e2b79899441fd30d1a702329"
   },
   "outputs": [],
   "source": [
    "cat_price_cnt = sales_cat.sort_values('date').groupby(['date_block_num','item_category_id']).agg({'item_price': 'mean', 'item_cnt_day': ['sum', 'mean']}).reset_index()\n",
    "cat_price_cnt.columns = cat_price_cnt.columns.droplevel().map(str)\n",
    "cat_price_cnt.columns = ['date_block_num', 'item_category_id', 'item_price_mean','item_cnt_month', 'item_cnt_mean' ]\n",
    "cat_price_cnt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73fa142d0e3995a7acd01af9fe641b947a8a091d"
   },
   "source": [
    "# Category type and item_type:\n",
    "Remember we extracted the `cat_type` and `item_type` from the `item_cat_df` dataframe. We can perform the same aggregation of price and item_cnt by these two new features. We wont go through the whole process of showing the time evolution of such aggregated features. Instead we limit ourselves to construct the dataframes that will be merged into the train dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0f99268b86e3c23a935b34bd5d53e0f71f3bd34"
   },
   "outputs": [],
   "source": [
    "sales_cat.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ddd3883e6b50c28a6287696bcb3d95e66003094"
   },
   "outputs": [],
   "source": [
    "cat_type_price_cnt = sales_cat.sort_values('date').groupby(['date_block_num','cat_type','cat_type_l']).agg({'item_price': 'mean', 'item_cnt_day': ['sum', 'mean']}).reset_index()\n",
    "cat_type_price_cnt.columns = cat_type_price_cnt.columns.droplevel().map(str)\n",
    "cat_type_price_cnt.columns = ['date_block_num', 'cat_type', 'cat_type_l', 'item_price_mean','item_cnt_month', 'item_cnt_mean' ]\n",
    "cat_type_price_cnt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c4de365f0dbe9c23217e2926fa0a6e7778509f2"
   },
   "outputs": [],
   "source": [
    "## This snippet has to be transformed into a function \n",
    "\n",
    "item_type_price_cnt = sales_cat.sort_values('date').groupby(['date_block_num','item_type','item_type_l']).agg({'item_price': 'mean', 'item_cnt_day': ['sum', 'mean']}).reset_index()\n",
    "item_type_price_cnt.columns = item_type_price_cnt.columns.droplevel().map(str)\n",
    "item_type_price_cnt.columns = ['date_block_num', 'item_type', 'item_type_l', 'item_price_mean','item_cnt_month', 'item_cnt_mean' ]\n",
    "item_type_price_cnt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1a8d5cad6dbb6707dff9fc0c6e68c647c727790"
   },
   "source": [
    "# By item_id:\n",
    "Let us now explore the `item_df` and see what features we extract. \n",
    "It has `['item_name','item_id','item_category_id']` I don't think it has relevant info (i don't want to include the name to the model for now)\n",
    "We already used this dataframe when creating the sales_train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a26f3924dd8228d0dcb252ff54b013f9c45512e"
   },
   "outputs": [],
   "source": [
    "sales_train_df.sort_values('date').groupby(['date_block_num', 'item_id']).agg({'item_price': ['mean', 'std'], 'item_cnt_day':['sum', 'mean', 'std']}).reset_index().head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7df33b45087d216de022216dcf7b7c562e5214f4"
   },
   "source": [
    "# Pipeline:\n",
    "1. Eliminate Outliers from the train set.\n",
    "2. Perform Feature Engineering.\n",
    "3. Merge with the test_df to include **missing values**.\n",
    "4. Construct the final train dataset, this is, properly fill **missing values**.\n",
    "5. Play with models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8c0d5e4358e7ec7893216e916ee0fd2a39f5c4a"
   },
   "source": [
    "We can easily identify outliers by `item_price` and `item_cnt_day`, let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61d0cc66ced7c92888a407a509703cbe9fd819be"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(10,6))\n",
    "# plt.figure(figsize=(10,4))\n",
    "# plt.xlim(-100, 3000)\n",
    "sns.boxplot(x=sales_train_df['item_cnt_day'], ax =ax[0], palette='Set3' ).set_title('item_cnt_day distribution')\n",
    "# plt.figure(figsize=(10,4))\n",
    "# plt.xlim(sales_train_df['item_price'].min(), sales_train_df['item_price'].max())\n",
    "sns.boxplot(x=sales_train_df['item_price'], ax = ax[1] ).set_title('item_price distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "14550b710af6ff28c66a634f585c74198d664610"
   },
   "outputs": [],
   "source": [
    "# There are repeated shops with different id let's correct this\n",
    "# Якутск Орджоникидзе, 56\n",
    "sales_train_df.loc[sales_train_df.shop_id == 0, 'shop_id'] = 57\n",
    "test_df.loc[test_df.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "sales_train_df.loc[sales_train_df.shop_id == 1, 'shop_id'] = 58\n",
    "test_df.loc[test_df.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "sales_train_df.loc[sales_train_df.shop_id == 10, 'shop_id'] = 11\n",
    "test_df.loc[test_df.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bc338c32da9dda3827b4e2431f0510cc6af2955"
   },
   "source": [
    "Less than 0.25% of rows are removed as outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f563dacf035a1c2df7c9cfdbe76b242e39bedf3"
   },
   "outputs": [],
   "source": [
    "(sales_train_df.shape[0]-sales_train_df.query('item_cnt_day > 0 and item_cnt_day <= 400 and item_price <= 40000').shape[0])/sales_train_df.shape[0] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f32e74b0e742516529fffda00d2e6ce9479dd76"
   },
   "outputs": [],
   "source": [
    "train_0 = sales_train_df.query('item_cnt_day > 0 and item_cnt_day <= 400 and item_price <= 40000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00f3bb8b83fe674910c4983ad7d0056461ababd0"
   },
   "source": [
    "Do we merge with test and fill 0 before doing feature engineering? If we do so, we are assuming the source of **missing values**  is the lack of sales for those (shop, item_id) pairs. I actually believe the opposite, the lack of data for these **missing values** is due to how I believe the train set was constructed. This is, as a random sample drawn from a bigger distribution. Therefore, I believe it is better to perform feature engineering first and then to merge carefully. To this purpose we will proceed as follows: \n",
    " * Construct a blank dataset containing the set of all pairs, that is, both pairs in train and test.\n",
    " * Perform feature engineering with train dataset only and merge with the blank_df step by step to include the aggregated features\n",
    " * There will be features that wont be filled in by the merge procedure, such as, item_price and item_cnt, those are to be estimated by the other features, such as, item_type_mean or median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b5a097122ade7e96bd6b92cc23f1b2b3d842eda"
   },
   "outputs": [],
   "source": [
    "# There are no new shops in the test_df dataset\n",
    "test_df['shop_id'].isin(train_0['shop_id']).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c424554ccd07bf971b7050da8fe257fcf1110e3"
   },
   "outputs": [],
   "source": [
    "# Number of unique items that are not in train dataset \n",
    "print('Number of unique items that are not in train: {}'.format(test_df[~test_df['item_id'].isin(train_0['item_id'])]['item_id'].unique().shape[0]))\n",
    "print('Number of rows that are not in train:\\t {}'.format(test_df[~test_df['item_id'].isin(train_0['item_id'])].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "08f1a1d31f67826d300b87a82c7c492642aaa000"
   },
   "outputs": [],
   "source": [
    "items_notin_train = test_df[~test_df['item_id'].isin(train_0['item_id'])]['item_id'].unique()\n",
    "all_items = np.concatenate((train_0['item_id'].unique(),items_notin_train))\n",
    "all_shops = train_0['shop_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35d153f4f7e6a56207c9bd9613e2a6b62fe49ca3"
   },
   "outputs": [],
   "source": [
    "blank_df = []\n",
    "for i in range(34):\n",
    "    for shop in all_shops:\n",
    "        for item in all_items:\n",
    "            blank_df.append([i, shop, item])\n",
    "    \n",
    "blank_df = pd.DataFrame(blank_df, columns=['date_block_num','shop_id','item_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c50032d8054865eea2aebe370cc887016c30884b"
   },
   "source": [
    "# Feature Engineering: \n",
    "## By item_id:  \n",
    "* mean encoded price and item_cnt_month\n",
    "* total and by shop_id\n",
    "* price increase or decrease by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb35a436def86f27f9e55c9c26d838d16afa54b0"
   },
   "outputs": [],
   "source": [
    "# The function below groups the dataframe by the set of columns in groups and aggregates the keys of dic_agg\n",
    "#  by the methods specified as the values of dic_agg\n",
    "\n",
    "def monthly_agg(df, groups, dic_agg = {'item_price': ['mean','std', 'max', 'min'], \n",
    "                                       'item_cnt_day': ['sum','count','mean', 'std']}, \n",
    "                sort_column = 'date', fill0 = False, rename_col = False):\n",
    "#     add the month column\n",
    "    groups = ['date_block_num'] + groups\n",
    "#     define auxiliar variable for column name changing\n",
    "    col_names = groups + [(str(key)+'_'+str(value)) for key in dic_agg.keys() for value in dic_agg[key]]\n",
    "#  back to business\n",
    "    g_df = df.sort_values(sort_column).groupby(groups, as_index = False)\n",
    "    agg_df = g_df.agg(dic_agg)\n",
    "    if fill0 == True:\n",
    "        agg_df.fillna(0, inplace= True)\n",
    "    if rename_col == True:\n",
    "        agg_df.columns = col_names\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1abc0afd79e95735b4fab12e06f14301975220a2"
   },
   "source": [
    "## By_item_id (monthly):\n",
    "    dic_agg = {'item_price': ['mean','std', 'max', 'min'], 'item_cnt_day': ['sum','count','mean', 'std']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3050e2a76bc055339b052bcd4365a0f5be4032bb"
   },
   "outputs": [],
   "source": [
    "# lets check our function, group by 'date_block_num' and 'item_id' then aggregate by 'item_price' and 'item_cnt_day'\n",
    "groups = ['item_id']\n",
    "# dic_agg = {'item_price': ['mean','std', 'max', 'min'], 'item_cnt_day': ['sum','count','mean', 'std']}\n",
    "agg_all_by_item = monthly_agg(train_0, groups, rename_col=True)\n",
    "agg_all_by_item.columns = ['date_block_num', 'item_id', 'item_price_mean', 'item_price_std',\n",
    "       'item_price_max', 'item_price_min', 'item_cnt_sum','item_cnt_count', 'item_cnt_mean', 'item_cnt_std']\n",
    "agg_all_by_item.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3353790c827b7767b13ea6e1d14adc0f4f8adb6e"
   },
   "source": [
    "## By: shop_id and item_id (monthly):\n",
    "    dic_agg = {'item_price': ['mean','std', 'max', 'min'], 'item_cnt_day': ['sum','count','mean', 'std']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "064cbedb4d7a8934de00995c80b526aab192549c"
   },
   "outputs": [],
   "source": [
    "groups = ['shop_id', 'item_id']\n",
    "agg_shop_item = monthly_agg(train_0, groups,rename_col= True)\n",
    "['date_block_num', 'shop_id', 'item_id', 'it_price_shop_mean',\n",
    "       'it_price_shop_std', 'it_price_shop_max', 'it_price_shop_min',\n",
    "       'it_cnt_shop_sum', 'it_cnt_shop_count', 'it_cnt_shop_mean',\n",
    "       'it_cnt_shop_std']\n",
    "agg_shop_item.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c22dc7b26fd44311a3f579091865ef91ccbc75e"
   },
   "source": [
    "## By: shop_id, cat_id and item_id (monthly):\n",
    "    dic_agg = {'item_price': ['mean','std', 'max', 'min'], 'item_cnt_day': ['sum','count','mean', 'std']}\n",
    " To do that we first extract the info about cat_type and item_type from the `item_cat_df` dataframe, then we label encode these features for later use, this was already done and it is stored in `item_cat_ext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd5319a9ecfadfd21d9c321f6695053f5df5e21f"
   },
   "outputs": [],
   "source": [
    "item_cat_exp.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3ba47600a86650c3a434a057702769268b6a403"
   },
   "outputs": [],
   "source": [
    "# Now, join this dataset with train_0\n",
    "train_0_cat = train_0.join(items_df, on = 'item_id', rsuffix='_').join(item_cat_exp, on = 'item_category_id', rsuffix = \"_\").drop(['item_name','item_id_','item_category_id_'], axis =1)\n",
    "train_0_cat.drop(['cat_type', 'item_type'], axis =1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82185a20be14facdefed51b7ea9cc20ffc6a0327"
   },
   "outputs": [],
   "source": [
    "#  Do the same of rht test_df, extend it:\n",
    "test_0 = test_df.join(items_df, on = 'item_id', rsuffix='_').join(item_cat_exp, on = 'item_category_id', rsuffix = \"_\").drop(['item_name','item_id_','item_category_id_'], axis =1)\n",
    "test_0.drop(['cat_type', 'item_type'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64fa2e46ce9b3da871e51656d8ca369ac3305848"
   },
   "outputs": [],
   "source": [
    "test_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd3dc45d965cf08f510b8aee2b0aada383457ce7"
   },
   "source": [
    "Now, we will use these extended features (regarding categories) to find aggregate values for price and (monthly) count by shop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11ca1747cdd606f4dc7000c02e8124c6da438b4a"
   },
   "outputs": [],
   "source": [
    "# group by shop and category_id, then aggregate price and cnt\n",
    "groups = ['shop_id','item_category_id']\n",
    "agg_shop_cat = monthly_agg(train_0_cat, groups, rename_col= True)\n",
    "agg_shop_cat.columns = ['date_block_num', 'shop_id', 'item_category_id', 'shop_cat_mean_p',\n",
    "       'shop_cat_std_p', 'shop_cat_max_p', 'shop_cat_min_p',\n",
    "       'shop_cat_cnt', 'shop_cat_cnt_count', 'shop_cat_cnt_mean',\n",
    "       'shop_cat_cnt_std']\n",
    "agg_shop_cat.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e932564b53ca43efcb6da307abb2c2efee7f20ba"
   },
   "outputs": [],
   "source": [
    "# group by shop and cat_type, then aggregate price and cnt\n",
    "groups = ['shop_id','cat_type_l']\n",
    "agg_shop_cat_type = monthly_agg(train_0_cat, groups, rename_col= True)\n",
    "agg_shop_cat_type.columns = ['date_block_num', 'shop_id', 'cat_type_l', 'shop_cat_type_mean_p',\n",
    "       'shop_cat_type_std_p', 'shop_cat_type_max_p', 'shop_cat_type_min_p',\n",
    "       'shop_cat_type_cnt', 'shop_cat_type_cnt_count', 'shop_cat_type_cnt_mean',\n",
    "       'shop_cat_type_cnt_std']\n",
    "agg_shop_cat_type.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ef1c46acd8fb88d7c75cc2e7cef75bc91927d00"
   },
   "outputs": [],
   "source": [
    "# Group by shop and item_type then aggregate price and count\n",
    "groups = ['shop_id','item_type_l']\n",
    "agg_shop_it_type = monthly_agg(train_0_cat, groups, rename_col= True)\n",
    "agg_shop_it_type.columns = ['date_block_num', 'shop_id', 'item_type_l', 'shop_it_type_mean_p',\n",
    "       'shop_it_type_std_p', 'shop_it_type_max_p', 'shop_it_type_min_p',\n",
    "       'shop_it_type_cnt', 'shop_it_type_cnt_count', 'shop_it_type_cnt_mean',\n",
    "       'shop_it_type_cnt_std']\n",
    "agg_shop_it_type.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "514480f9b3760e2696210a2ca8ef42431dfa3b92"
   },
   "outputs": [],
   "source": [
    "# Group by shop and item_type then aggregate price and count\n",
    "groups = ['shop_id','item_category_id', 'cat_type_l']\n",
    "agg_shop_cat_id_type = monthly_agg(train_0_cat, groups, rename_col= True)\n",
    "agg_shop_cat_id_type.columns = ['date_block_num', 'shop_id', 'item_category_id' , 'cat_type_l', 'shop_cat_id_type_mean_p',\n",
    "       'shop_cat_id_type_std_p', 'shop_cat_id_type_max_p', 'shop_cat_id_type_min_p',\n",
    "       'shop_cat_id_type_cnt', 'shop_cat_id_type_cnt_count', 'shop_cat_id_type_cnt_mean',\n",
    "       'shop_cat_id_type_cnt_std']\n",
    "agg_shop_cat_id_type.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5927b5073b042d79311d0ab3da420340fce1f2bd"
   },
   "outputs": [],
   "source": [
    "# Group by shop and item_type then aggregate price and count\n",
    "groups = ['shop_id','item_category_id', 'item_type_l']\n",
    "agg_shop_cat_it_type = monthly_agg(train_0_cat, groups, rename_col= True)\n",
    "agg_shop_cat_it_type.columns = ['date_block_num', 'shop_id', 'item_category_id' , 'item_type_l', 'shop_cat_it_type_mean_p',\n",
    "       'shop_cat_it_type_std_p', 'shop_cat_it_type_max_p', 'shop_cat_it_type_min_p',\n",
    "       'shop_cat_it_type_cnt', 'shop_cat_it_type_cnt_count', 'shop_cat_it_type_cnt_mean',\n",
    "       'shop_cat_it_type_cnt_std']\n",
    "agg_shop_cat_it_type.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40a60c69de5ff59cd69d8a48395466eed2739228"
   },
   "outputs": [],
   "source": [
    "#  this is doing nothing !!!\n",
    "groups = ['shop_id', 'item_id', 'item_category_id', 'cat_type_l', 'item_type_l']\n",
    "agg_shop_cat_it = monthly_agg(train_0_cat, groups, rename_col= True)\n",
    "agg_shop_cat_it.columns = ['date_block_num', 'shop_id', 'item_id', 'item_category_id',\n",
    "       'cat_type_l', 'item_type_l', 'it_price_shop_cat_mean', 'it_price_shop_cat_std',\n",
    "       'it_price_shop_cat_max', 'it_price_shop_cat_min', 'it_cnt_shop_cat_sum',\n",
    "       'item_cnt_shop_cat_count', 'item_cnt_shop_cat_mean', 'item_cnt_shop_cat_std']\n",
    "agg_shop_cat_it.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "ed59961724fb666ca6b30966df62e7262be75aa6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>cat_type_l</th>\n",
       "      <th>item_type_l</th>\n",
       "      <th>it_price_shop_cat_mean</th>\n",
       "      <th>it_price_shop_cat_std</th>\n",
       "      <th>it_price_shop_cat_max</th>\n",
       "      <th>it_price_shop_cat_min</th>\n",
       "      <th>it_cnt_shop_cat_sum</th>\n",
       "      <th>item_cnt_shop_cat_count</th>\n",
       "      <th>item_cnt_shop_cat_mean</th>\n",
       "      <th>item_cnt_shop_cat_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1608206</th>\n",
       "      <td>33</td>\n",
       "      <td>59</td>\n",
       "      <td>22087</td>\n",
       "      <td>83</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608207</th>\n",
       "      <td>33</td>\n",
       "      <td>59</td>\n",
       "      <td>22088</td>\n",
       "      <td>83</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608208</th>\n",
       "      <td>33</td>\n",
       "      <td>59</td>\n",
       "      <td>22091</td>\n",
       "      <td>83</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608209</th>\n",
       "      <td>33</td>\n",
       "      <td>59</td>\n",
       "      <td>22100</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>629.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>629.0</td>\n",
       "      <td>629.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608210</th>\n",
       "      <td>33</td>\n",
       "      <td>59</td>\n",
       "      <td>22102</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_block_num          ...            item_cnt_shop_cat_std\n",
       "1608206              33          ...                              1.0\n",
       "1608207              33          ...                              0.0\n",
       "1608208              33          ...                              NaN\n",
       "1608209              33          ...                              NaN\n",
       "1608210              33          ...                              NaN\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_shop_cat_it.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2bae7f5da9ad19ffcfa38e2b516c22a86a1aaa0f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19cb19a1e4c60bfaee46206e92f1cf147b761367"
   },
   "outputs": [],
   "source": [
    "# price and count aggregation by month -we always do it monthly - and item_id (so, we're lookind at the whole store globally) \n",
    "g_price_train_0 = train_0.sort_values('date').groupby(['date_block_num','item_id'], as_index = False)\n",
    "agg_price_train_0 = g_price_train_0.agg({'item_price': ['mean','std', 'max', 'min'], 'item_cnt_day': ['sum','count','mean', 'std']})\n",
    "agg_price_train_0.columns = ['date_block_num', 'item_id', 'item_mean_pr','item_std_pr','item_max_pr','item_min_pr', 'item_all_cnt','item_all_transactions', 'item_all_mean_cnt','item_std_all_cnt']\n",
    "agg_price_train_0.fillna(0, inplace = True)\n",
    "agg_price_train_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af01df531bcc34931482bea17c8e3794b14a8940"
   },
   "outputs": [],
   "source": [
    "pre_train_by_item = pd.merge(blank_df, agg_all_by_item, on=['date_block_num', 'item_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "375322b871f7faf15fcc0974d4d13512a9279ed8"
   },
   "outputs": [],
   "source": [
    "pre_train_by_item[pre_train_by_item.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afa5c0a742cb51412f51d8917ebc60d4ccca99fd"
   },
   "outputs": [],
   "source": [
    "#  Not just yet, lets perform feature engineering before doing this.\n",
    "# train_1 = blank_df.join(items_df, on='item_id', rsuffix='_').join(shops_df, on='shop_id', rsuffix='_').join(item_cat_df, on='item_category_id', rsuffix='_').drop(['item_id_', 'shop_id_', 'item_category_id_'], axis=1)\n",
    "# train_1.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "074339996194e8c040d8f516d2ee4e5785d0a9e7"
   },
   "outputs": [],
   "source": [
    "train_by_month = train_1.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id'], as_index=False)\n",
    "train_by_month = train_by_month.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\n",
    "# Rename features.\n",
    "train_by_month.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions']\n",
    "train_by_month.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "107e56708dfc76a2f4fc536a36c0e5354e05eaba"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7c7ce07ae8a05bd6ed422056ee40f62c7739563"
   },
   "outputs": [],
   "source": [
    "pd.merge(blank_df, train_by_month, on=['date_block_num','shop_id','item_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb52f35f8852222945866de3debd542e68f4c212"
   },
   "outputs": [],
   "source": [
    "print('train shape: {}'.format(train.shape))\n",
    "print('time period:\\n\\t start -> {} \\n\\t  end -> {} '.format(train['date'].min().date(), train['date'].max().date()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2845252b8577538f4faea689ff077b49f502cc05"
   },
   "source": [
    "Let us look for items that were not sold during the last 6 months. \n",
    " * 12391 out of 21807 (~57%) of items have item_cnt_month = 0, this is, no sells. \n",
    " * Do we get rid of these items?  I don't think so, the model has to see pairs with 0 sells (I think). They're part of the distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8146a587510890759a2d6495e428d341cca46865"
   },
   "outputs": [],
   "source": [
    "sales_by_item_id = sales_train_df.pivot_table(index=['item_id'],values=['item_cnt_day'], \n",
    "                                        columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\n",
    "sales_by_item_id.columns = sales_by_item_id.columns.droplevel().map(str)\n",
    "sales_by_item_id = sales_by_item_id.reset_index(drop=True).rename_axis(None, axis=1)\n",
    "sales_by_item_id.columns.values[0] = 'item_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc69c94af0874825121aabade6e20bbbdd5ccd05"
   },
   "source": [
    "Predictions are asked at a monthly level (for the 34th month) which encompass Dec15/Jan16, so we might have a very strong seasonal component.\n",
    "\n",
    "Let's group data by month > shop_id > item_category_id > item_id and then aggregate data concerning sales and price, this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b40acb43e9a978f7d430a6f9de24ed76ff009bde",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_month = train.sort_values('date').groupby(['date_block_num','shop_id', 'item_category_id', 'item_id'], as_index = False).agg({'item_price': 'mean', 'item_cnt_day':['sum', 'mean','count']})\n",
    "train_month.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions']\n",
    "train_month.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0dbf95c56ed31977b288f4a033e5e3aee46fad0"
   },
   "outputs": [],
   "source": [
    "train_month.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fffa3c229a1f16cf248100d290fe44e02e9cd39c"
   },
   "outputs": [],
   "source": [
    "shop_unique = pd.DataFrame(sorted(train_month['shop_id'].unique()))\n",
    "item_unique = pd.DataFrame(sorted(train_month['item_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63fe995bafc56a3bfecbc0d3d55ac802e73a80a7"
   },
   "outputs": [],
   "source": [
    "train_month['shop_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3b369c7dc10857ee4cc5d9aa3ed91c461ca25c8"
   },
   "outputs": [],
   "source": [
    "# Merge the train set with the complete set (missing records will be filled with 0).\n",
    "train_month = pd.merge(blank_df, train_month, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "train_month.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af2d41d4d5219f71a4f2d02d0ab7af59073340fd"
   },
   "outputs": [],
   "source": [
    "# we need to fill this database more carefully, at least we should retrieve the item_category_id for each pair. The above code is just filling in 0 for every NaN value\n",
    "# this cannot be good for the model, but let's continue for the purpose of having a first complete pipeline .\n",
    "train_month.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f131412761834df3a4fa7691e1a346bdb9399cee"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c75d8458ccff859f331782cdae70070b3007bb35"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
